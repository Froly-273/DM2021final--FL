# Privacy in Data Mining: Federated Learning

## 模型选择

* node2vec
* 是一个two-staged模型，上游任务负责采样出很多条随机游走的轨迹，下游任务根据这些轨迹调用word2vec进行embedding

## non-IID采样

* 方法一：正常读数据集、建图（一张图）、跑随机游走，生成一个`random_walk_trace.txt`文件。然后，不管是shuffle也好，一次切一整片也好，把这些随机游走的traces分给K个client
* 方法二：在第一步读数据集的时候就有意的分成K个不相交的部分，这样建图建出来就是K张图，然后每张图上分别跑随机游走
  
## 设计一个会攻击所选模型的算法

* 它的存在意义是测试我们的模型是否真的保护了隐私
* 比如说，通过权重（梯度等）逆推出数据本身，这个要调研
* 在node2vec模型中，上下游任务是解耦的，所以我们只能期望攻击者通过获取word2vec的梯度，反推出随机游走采样后的chain，并不能直接获得原图中的边连接关系。
但是，由于这些chain内部是连接的，打个比方，1123 96 355这条链就代表着在原图中1123->96、96->355，
所以，仍然可以根据这些chain反推出原图的连接关系
* 但是你要注意,模型对攻击者来说是黑盒的,你不可以假设他是使用skipgram做embedding的

## shadow learning

```shell
Membership Inference Attacks Against Machine Learning Models
```
指出一种攻击方法：训练一个分类器,我可以告诉你这个样本是否在训练集中,从而反推出训练集

模型首先找到一个样本x,观察它经过目标模型的输出y,使用y来训练分类器,分类器的输出是1(样本x在训练集中)或者0(样本x不在训练集中).

如何找到样本x?文中提出的方法叫做影子模型:随机初始化x,若x在目标模型上的输出y的值高于某个阈值th(对于分类任务,这个y对应的类别概率首先得是最高的,其次它得比阈值高),那么就保留x(此时可以认为生成的x与训练集中的很像了)
,否则就随机改变x中的某k个特征,重新预测.这里k控制了收敛速度(但不一定收敛,有可能迭代完了之后还找不到这样的样本).

## DLG攻击

```shell
Deep Leakage From Gradients (NIPS 2019)
```
* 需要获取模型权重和梯度,就能预测出输入

指出一种攻击方法:
* 初始化虚拟输入`x`和虚拟标签`y`
* x经过模型后产生一个输出`y'`
* 使用`y'`和`y`计算loss得到虚拟梯度`G'`
* 用虚拟梯度`G'`和真实梯度`G`之间的误差作为DLG的loss，用这个loss指导虚拟输入(求loss对x的偏导)和虚拟标签(求loss对y的偏导)


* Q：是在训练过程还是测试过程中获得的梯度？
* A：当然是测试过程啊，这个是训练好的模型
* Q：如何实现获取测试过程中的梯度？
* A：善用autograd
```shell
y = model(x)
grad = torch.autograd.grad(y, model.parameters())
grad = list(g.detach().copy() for g in grad) 
```

必须要提到一点，作者的DLG是针对图像做攻击的，这可以make sense，因为图像是连续的。
但是，我们word2vec的任务是针对不连续的文本，所以无法直接apply在输入的句子上。
在作者的原论文中，他们是对embedding做攻击的，然后通过逆向embbedding矩阵求出哪个词离这个embedding最近。

我怎么感觉不行呢。他这个是embedding过后又经过了很复杂的NLP网络，它去攻击这个网络，但是我现在要攻击的就是embedding自身啊。
这就相当于说我要攻击的是一个“输入=输出”的模型，这不是很蠢吗？

## 基于embedding的泄露

在训练时，embedding矩阵作为模型参数与模型一起优化。embedding层的梯度是关于输入稀疏的，也就是说，给定text的一个batch，词向量只有在词出现在这个batch中才会更新。其他词的梯度为零。

这好像可以直接估计出来哪个词出现在training set里了。

## 下游任务embedding的隐私化

* 用DP(Differential Privacy)给输出加噪声
* 用安全聚合,这个方法也是加噪声,不过加的是零和随机噪声.server知道发给每个client的噪声量,它们是零和的,这样就算单个client在上传时被攻破了也没事

## 关于DP的讲解

```shell
https://zhuanlan.zhihu.com/p/40760105
```
讲的非常详细。如果我们只是需要`exp(e)`型的差分隐私，使用Laplace噪声足够；但如果是`exp(e)+d`型的，则需要Gaussian噪声。

## 实验流程

* 第零，baseline实验。lab2的模型测试跑分
* 第一，基本的FedAvg实验。测试跑分
* 第二，DLG攻击者实验。使用DLG去试图攻破训练好的模型，并试图复原出训练集，来模拟隐私泄露。测试DLG复原出训练集的跑分
* 第三，FedAvg防御实验。使用隐私方法，如DP、安全聚合等，去防御FedAvg模型，测试跑分，同时也要测试DLG的跑分。期望结果是跑分(精度)可能会受影响，但是DLG的跑分(隐私泄露程度)也会下降